{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PPGI_UFRJ](imagens/ppgi-ufrj.png)\n",
    "# Fundamentos de Ciência de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "[![DOI](https://zenodo.org/badge/335308405.svg)](https://zenodo.org/badge/latestdoi/335308405)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PPGI/UFRJ 2020.3/2022.2\n",
    "## Prof Sergio Serra e Jorge Zavaleta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Módulo 4 - Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> O trabalho de pré-processamento de dados para **aprendizado de máquina** pelo geral leva mais tempo e está relacionado à  limpeça de dados incorretos ou falta dos mesmos. Algumas estimativas sugerem que é gasto cerca de dois terços do tempo previsto limpando e organizando os conjuntos de dados. Esta abordagem inclui tarefas como:\n",
    "\n",
    "> - Mesclar os datasets em campos comuns para reunir todos os dados em uma única tabela.\n",
    "> - Engenharia de recursos para melhorar a qualidade dos dados.\n",
    "> - Remover ou preencher valores incorretos ou ausentes, por exemplo, substituindo os dados ausentes pela média ou mediana dos valores existentes para esse campo.\n",
    "> - Eliminando registros duplicados.\n",
    "> - Construir os conjuntos de dados de treinamento padronizando ou normalizando os dados necessários, aplicações para transformas as características e fazer divisão da população em amostras para treinamento, teste e validação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn\n",
    "> A biblioteca **scikit-learn** contém uma coleção de algoritmos relacionados ao aprendizado de máquina, incluindo regressão, classificação, redução de dimensionalidade e agrupamento. \n",
    "> - Conta com ferramentas simples e eficientes para análise preditiva de dados.\n",
    "> - Acessível a todos e reutilizável em vários contextos.\n",
    "> - Desenvolvida em conjunto com NumPy, SciPy e matplotlib.\n",
    "> - Código aberto, utilizável comercialmente - licença BSD\n",
    "> - Maiores informações em [Scikit-learn](http://scikit-learn.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizado Supervisionado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Regressão Linear\n",
    "> Uma das técnicas de aprendizado de máquina mais simples é a família de **modelos de regressão**. \n",
    "> O **modelo de regressão** é usado para prever uma **resposta** contínua e variável baseada em um conjunto de **caracteristicas**.\n",
    "\n",
    "> A regressão linear assume que a relação entre as **caracteristicas (entradas)** e o **vetor objetivo** é **aproximadamente linear**. O **objetivo** do modelo linear é encontrar os valores dos parâmetros/características que criarão uma linha reta (**y**) que melhor se ajuste aos dados.\n",
    "\n",
    "> ### $ y = w_{0} + w_{1}x_{1}+ w_{2}x_{2} +\\epsilon $ \n",
    "> onde: $w$ são denominados pesos (coeficientes) indentificados ao ajustar o modelo, $x_{i}$ dado da caracteristica, $\\epsilon$ é o error e $y$ é o vetor objetivo.\n",
    "\n",
    "> Se pode definir a função objetivo $J_{w}$ (Jacobiano) para minimizar:\n",
    "> ## $minJ_{w} = \\frac{1}{2m}\\sum_{i=1}^{m}(y_{w}(x^{(i)}) - y^{(i)})^{2}$\n",
    "> onde: **m** número de amostras de treianemnto, $y_{w}(x^{(i)})$ valor estimando na $i^{th}$ amostra e $y^{i}$ é valor atual. \n",
    "\n",
    "> Esta é a função de custo de **y** (mede o custo do erro, quanto maior o erro, maior o custo). Este método também é conhecido como **sum of the squared error (SSE)** e outras varientes como **root mean square error (RMSE)** e **mean square error (MSE)** usados segundo seja o caso a ser analisado.\n",
    "\n",
    "> ### $MSE = \\frac{\\sum_{i=1}^{n}(y_{p}-y)^{2}}{n}$, $MAE = \\frac{\\sum_{i=1}^{n}|y_{p}-y|}{n} $, $R^{2}=1- \\frac{\\sum_{i=1}^{n}(y - y_{p})^{2}}{\\sum_{i=1}^{n}(y - \\bar{y}_{p}))^{2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando as bibliotecas\n",
    "import numpy as np                                        # numpy\n",
    "import matplotlib.pyplot as plt                           # graficos\n",
    "import pandas as pd                                       # pandas\n",
    "from sklearn.model_selection import train_test_split      # particionar o dataset: treinamento, teste\n",
    "from sklearn.linear_model import LinearRegression         # regressão linear\n",
    "from sklearn.preprocessing import StandardScaler          # preprocessamento estandar\n",
    "import seaborn as sns                                     # seaborn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exemplo regressão linear\n",
    "sns.set()\n",
    "# gerando os dados aleatoriamente\n",
    "valores = np.random.RandomState(1)\n",
    "x = 10 * valores.rand(50)\n",
    "y = 2 * x - 5 + valores.randn(50)\n",
    "#cria a figura\n",
    "fig1 = plt.figure(figsize=(8,6))\n",
    "#\n",
    "plt.scatter(x, y,color='red', s=50)\n",
    "plt.show()\n",
    "#fig1.savefig('imagens/reg0.png') # gravar a figura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando a regressao linear\n",
    "modelo = LinearRegression(fit_intercept=True)\n",
    "#\n",
    "# ajusta os valores no modelo (fit) --> aprendizagem\n",
    "modelo.fit(x[:, np.newaxis], y)\n",
    "#\n",
    "# gera valores de ajuste\n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "#\n",
    "# predice os valores usando os valores de ajuste  --> predição\n",
    "yfit = modelo.predict(xfit[:, np.newaxis])\n",
    "#\n",
    "# gera a gráfico\n",
    "fig2 = plt.figure(figsize=(8,6))\n",
    "plt.scatter(x, y, s=50, color='red')     # plota os pontos\n",
    "plt.plot(xfit, yfit,'k',linewidth=4,label='y = 2.027x - 4.99') # plota a linha reta\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#fig2.savefig('imagens/reg1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $w_{0}$ é chamado de *bias* ou *interceptação* e poder ser visualizado usndo **intercept_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualiza a interseção da linha reta\n",
    "w0 = modelo.intercept_\n",
    "print(\"Interseção da linha reta: w0 =\",w0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> E os pesos $w_{1}$ e $w_{2}$ são visualizados usando **coef_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualiza os pesos\n",
    "w1 = modelo.coef_[0]\n",
    "print(\"Pesos: w1 = \", w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### linha reta da regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linha reta\n",
    "print('Linha reta :', 'y =',w1,'x',w0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Dataset: Preços das casa em Boston (pre-instalado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura dos dados\n",
    "from sklearn.datasets import load_boston\n",
    "import warnings\n",
    "#\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    X, y = load_boston(return_X_y=True)\n",
    "#\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "#boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x5 = X[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descrição do dataset\n",
    "#print(boston.DESCR)              # descrção do dataset\n",
    "#print(boston.feature_names)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Particionado em entrada e saida\n",
    "#x = boston.data                  # vetor de entrada\n",
    "#\n",
    "#y = boston.target               # vetor objetivo (preços das casas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Particionando o dataset por número médio de quartos por habitação (RM)\n",
    "x5 = X[:,5]\n",
    "#x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grafico\n",
    "fig3 = plt.figure(figsize=(10,8))\n",
    "plt.scatter(x5, y, s=50, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gerando o modelo de regressão\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando a matriz 2D em 1D\n",
    "xt = np.transpose(np.atleast_2d(x5))\n",
    "#\n",
    "# ajusta os valores no modelo\n",
    "lr.fit(xt, y)\n",
    "#\n",
    "# predição dos preços dos custos das casas\n",
    "y_predicted = lr.predict(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grafico\n",
    "fig4 = plt.figure(figsize=(10,8))\n",
    "plt.scatter(x5, y, s=50, color='red')     # plota os pontos\n",
    "plt.plot(xt, y_predicted,'k',linewidth=4, label='y = 9.102x - 34.67') # plota a linha reta\n",
    "plt.xlabel('Casas em Boston')\n",
    "plt.ylabel('Precios das casas')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#fig4.savefig('imagens/reg2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualiza a interseção da linha reta\n",
    "print(\"Interseção da linha reta:\", lr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualiza os pesos\n",
    "print(\"Pesos da linha reta (w): \", lr.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linha reta\n",
    "print('Linha reta :', 'y =',lr.coef_[0],'x',lr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  O gráfico anterior mostra os pontos e o ajuste (linha reta). Visualmente parecem estar bem distribuidos, no entanto, observa-se que existem **outliers**. \n",
    "\n",
    "> Idealmente, entretanto, se pode medir o quão bom é um ajuste quantitativo com a finalidade de comparar métodos alternativos. Isto pode ser feito medindo o quão perto a previsão está dos valores verdadeiros. Este procedimento pode ser feito usando **MSE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usando o MSE do scikit-learn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#\n",
    "#mse = mean_squared_error(y, lr.predict(xt))\n",
    "mse = mean_squared_error(y, y_predicted)    # erro = y - yp\n",
    "print(\"Mean Squared Error (MSE) = {:.3}\".format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tambem se pode usar RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"RMSE = {:.3}\".format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O coeficiente de determinação pode ser obtido usando r2_score\n",
    "from sklearn.metrics import r2_score\n",
    "#\n",
    "r2 = r2_score(y,y_predicted)        # y - yp\n",
    "print(\"R2 = {:.2}\".format(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternativa: calculo dos coeficientes de determianação - opção\n",
    "r2 = lr.score(xt,y)\n",
    "print(\"R2 = {:.2}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Visualização das curvas de treinamento e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# particionando o dataset em treiamento e teste\n",
    "Xb_train, Xb_test, yb_train, yb_test = train_test_split(X,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando o modelo\n",
    "lr_boston =  LinearRegression()\n",
    "#\n",
    "# ajustando os valores\n",
    "fit_boston = lr_boston.fit(Xb_train,yb_train)\n",
    "#\n",
    "print('Escore da Regressão R2: {:.4f}'.format(lr_boston.score(Xb_test,yb_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicção\n",
    "yb_pred = lr_boston.predict(Xb_test)\n",
    "e_boston = yb_pred-yb_test\n",
    "print('RMSE: {:.4f}'.format(np.sqrt(mean_squared_error(yb_test,yb_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calulda o SSE (sum of squared errors)\n",
    "def sse(residual):\n",
    "    return np.sum(np.power(residual,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculando os residuais\n",
    "resid_train = yb_train - lr_boston.predict(Xb_train)\n",
    "print('Residual Treinamento: ',sse(resid_train))\n",
    "resid_test = yb_test - lr_boston.predict(Xb_test)\n",
    "print('Residual Teste: ',sse(resid_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternativa: medir o ajuste do modelo para o dataset é R2 (R-squared score)\n",
    "print('Treinamento:',lr_boston.score(Xb_train, yb_train))\n",
    "print('Test',lr_boston.score(Xb_test, yb_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculos auxiliares\n",
    "alfas = np.logspace(-6, 2, 100)\n",
    "coeffs = np.zeros((len(alfas), Xb_train.shape[1]))   # matriz de zeros\n",
    "sse_train = np.zeros_like(alfas)\n",
    "sse_test = np.zeros_like(alfas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando outro modelo de regularização\n",
    "from sklearn import linear_model                           # Modelo linear\n",
    "# alfa=2.5\n",
    "#lm_boston = linear_model.Ridge(alpha=2.5)\n",
    "#linear_model.Ridge(alpha=2.5)\n",
    "for n, alpha in enumerate(alfas):\n",
    "    model_boston = linear_model.Lasso(alpha=alpha)\n",
    "    model_boston.fit(Xb_train, yb_train)\n",
    "    coeffs[n, :] = model_boston.coef_\n",
    "    sse_train[n] = sse(yb_train - model_boston.predict(Xb_train))\n",
    "    sse_test[n] = sse(yb_test - model_boston.predict(Xb_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, b = plt.subplots(1, 2, figsize=(12, 4), sharex=True)\n",
    "for n in range(coeffs.shape[1]):\n",
    "    b[0].plot(np.log10(alfas), coeffs[:, n], color='k', lw=1)\n",
    "    #\n",
    "b[1].semilogy(np.log10(alfas), sse_train, 'b', lw=2, label=\"treinamento\")\n",
    "b[1].semilogy(np.log10(alfas), sse_test, 'r',lw=2, label=\"teste\")\n",
    "b[1].legend(loc=0)\n",
    "#\n",
    "b[0].set_xlabel(r\"${\\log_{10}}\\alpha$\", fontsize=18)\n",
    "b[0].set_ylabel(r\"coeficientes\", fontsize=18)\n",
    "b[1].set_xlabel(r\"${\\log_{10}}\\alpha$\", fontsize=18)\n",
    "b[1].set_ylabel(r\"sse\", fontsize=18)\n",
    "#\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## dataset: Diabetes\n",
    ">> Este dataset pode ser encontrado no Kaggle [Diabetes](https://www.kaggle.com/saurabh00007/diabetescsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dataset diabetes\n",
    "diabetes_data = pd.read_csv('data/diabetes.csv')\n",
    "diabetes_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processando os dados\n",
    "df_diabetes=pd.DataFrame(diabetes_data)\n",
    "#df_diabetes.head()\n",
    "# Creating a new column containing response variable 'y' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a matriz de caracteristicas\n",
    "XD = df_diabetes.iloc[:,:-1]\n",
    "#print(XD.head())\n",
    "# Criando o vetor objetivo (Outcome)\n",
    "yd = df_diabetes.iloc[:,-1]\n",
    "#print(yd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualizar por pares\n",
    "#sns.set()\n",
    "sns.pairplot(df_diabetes, hue='Outcome', height=1.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escolher BMI -. variavel de entrada\n",
    "bmi = np.array(XD['BMI'])\n",
    "bmi = bmi[:,np.newaxis]\n",
    "#\n",
    "# diabetes: objetivo (Outcome)\n",
    "yd = np.array(df_diabetes.iloc[:,-1])\n",
    "yd = yd[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando o modelo linear\n",
    "diabetes_lr = LinearRegression()\n",
    "#\n",
    "# Ajuste do modelo\n",
    "diabetes_lr.fit(bmi,yd)\n",
    "#\n",
    "# prediction\n",
    "yd_predicted = diabetes_lr.predict(bmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regressao linear (BMI= Body Mass Index)\n",
    "diabetes = plt.figure(figsize=(10,6))\n",
    "plt.scatter(bmi, yd, color='blue', s=50)\n",
    "plt.plot(bmi, yd_predicted, c='r', linewidth=4, label='y = 0.0161x - 0.176')\n",
    "plt.title('Regressão Linear - Diabetes')\n",
    "plt.ylabel(\"Diabetes:[Sim, Não]\")\n",
    "plt.xlabel(\"BMI: Índice de Massa Muscular\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#diabetes.savefig('imagens/diabetes.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "mse = cross_val_score(diabetes_lr,bmi,yd,scoring='neg_mean_squared_error',cv=10)\n",
    "# mean(): indicador de bom modelo\n",
    "mse.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linha reta\n",
    "print('Linha reta :', 'y =',diabetes_lr.coef_[0][0],'x',diabetes_lr.intercept_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# particionando o dataset em treiamento e teste\n",
    "Xd_train, Xd_test, yd_train, yd_test= train_test_split(bmi,yd,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando o modelo\n",
    "lr_diabetes =  LinearRegression()\n",
    "#\n",
    "# ajustando os valores\n",
    "fit_diabetes = lr_diabetes.fit(Xd_train,yd_train)\n",
    "#\n",
    "print('Escore da Regressão R2: {:.4f}'.format(lr_diabetes.score(Xd_test,yd_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculando os residuais\n",
    "resid_train_dia = yd_train - lr_diabetes.predict(Xd_train)\n",
    "print('Residual Treinamento: ',sse(resid_train_dia))\n",
    "resid_test_dia = yd_test - lr_diabetes.predict(Xd_test)\n",
    "print('Residual Teste: ',sse(resid_test_dia))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternativa: medir o ajuste do modelo para o dataset é R2 (R-squared score)\n",
    "print('Treinamento:',lr_diabetes.score(Xd_train, yd_train))\n",
    "print('Test',lr_diabetes.score(Xd_test, yd_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Classificação\n",
    ">> A classificação é uma técnica que permite saber se um elemento pertence a uma determianada classe. Se existem duas classes como elementos sim/não, verdadeiro/falso, então é denominada de **classifcação binária**.\n",
    ">> Os algoritmos usados com maior frequência são:\n",
    ">> - Regressão logística\n",
    ">> - Árvores de decisão\n",
    ">> - SVM - Support Vertor Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Dataset: Rede Social\n",
    ">> Dataset encontrado no Kaggle [dataset](https://www.kaggle.com/rakeshrau/social-network-ads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redes Sociais\n",
    "rede_social = pd.read_csv('data/Social_Network_Ads.csv',delimiter=',')\n",
    "rede_social.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conjuntos de Treinamento (X) e Teste (y)\n",
    "X = rede_social.iloc[:, [2, 3]].values # treinamento colunas Age e EstimatedSalary\n",
    "y = rede_social.iloc[:, 4].values # teste (comprado ou não)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cortando em conjuntos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3,random_state = 0)\n",
    "print('X-train:',X_train.shape)\n",
    "print('y-train:',y_train.shape)\n",
    "print('X-test:',X_test.shape)\n",
    "print('y-test:',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# escalonamento das caracteristicas\n",
    "rs_sc = StandardScaler()\n",
    "X_train = rs_sc.fit_transform(X_train)     # ajusta os dados ao modelo\n",
    "X_test = rs_sc.transform(X_test)           # std dos dados\n",
    "#print('X_train:',X_train)\n",
    "#print('X_test:',X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### Classificação usando Regressão Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Logistic Regression to the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "#\n",
    "classificador_rs = LogisticRegression(random_state = 0)\n",
    "classificador_rs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicão dos resultados do conjunto de teste\n",
    "yrs_pred = classificador_rs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculando os residuais\n",
    "# calulda o SSE (sum of squared errors)\n",
    "def sse_rs(residual):\n",
    "    return np.sum(np.power(residual,2))\n",
    "#\n",
    "resid_train_rs = y_train - classificador_rs.predict(X_train)\n",
    "print('Residual Treinamento RS =',sse_rs(resid_train_rs))\n",
    "resid_test_rs = y_test - classificador_rs.predict(X_test)\n",
    "print('Residual Teste RS =',sse_rs(resid_test_rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternativa: medir o ajuste do modelo para o dataset é R2 (R-squared score)\n",
    "print('Treinamento RS =',classificador_rs.score(X_train, y_train))\n",
    "print('Test RS =',classificador_rs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando a matriz de confusão\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, yrs_pred)\n",
    "#\n",
    "plt.figure(figsize=(4,3)) \n",
    "sns.heatmap(cm.T, annot=True, cmap='gist_ncar_r')\n",
    "plt.title('Matriz de confusão da Regressão logística',y=1.05, size=13)\n",
    "plt.xlabel('Valores verdadeiros')\n",
    "plt.ylabel('Valores predecidos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the Training set results\n",
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_train, y_train\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, \n",
    "                               step=0.01),np.arange(start = X_set[:, 1].min() - 1, \n",
    "                                                    stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "# gravar a figura\n",
    "classe0 = plt.figure(figsize=(10,8))\n",
    "#\n",
    "plt.contourf(X1, X2, classificador_rs.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('#B0E0E6', '#FF6347')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0 ], X_set[y_set == j, 1], c = ['blue', 'red'][i], label = j)\n",
    "plt.title('Classificação usando Regressão Logistica (CT: 0 = Não compra, 1 = compra)')\n",
    "plt.xlabel('Idade')\n",
    "plt.ylabel('Salario Estimado')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#classe0.savefig('imagens/classe0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar os resultados do teste\n",
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_test, y_test\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, \n",
    "                               step =0.01), \n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, \n",
    "                               step = 0.01))\n",
    "# gravar a figura\n",
    "classe1 = plt.figure(figsize=(10,8))\n",
    "#\n",
    "plt.contourf(X1, X2, classificador_rs.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "alpha = 0.75, cmap = ListedColormap(('#B0E0E6', '#FF6347')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "    c = ['blue', 'red'][i], label = j)\n",
    "plt.title('Classificação usando Regressão Logistica (Conjunto de Teste: 0 = Não compra, 1 = compra)')\n",
    "plt.xlabel('Idade')\n",
    "plt.ylabel('Salário estimado')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#classe1.savefig('imagens/classe1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Classificação usando SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando o classificador SVC (Support Vertorial Classifier)\n",
    "from sklearn.svm import SVC\n",
    "#\n",
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(X, y, test_size = 0.3, random_state=0)  # split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classificador\n",
    "scaler = StandardScaler()                          # modelo\n",
    "#\n",
    "X_train_std = scaler.fit_transform(Xr_train)       # ajuste ao treinamento\n",
    "X_test_std = scaler.fit_transform(Xr_test)         # ajuste ao teste\n",
    "#\n",
    "svm = SVC(gamma='auto').fit(X_train_std, yr_train)   # svm\n",
    "#svm = SVC(kernel='rbf', random_state=0, gamma=.10, C=1.0).fit(X_train_std, yr_train)\n",
    "#\n",
    "Xr_pred = svm.predict(X_train_std)                   # predição do treinamento\n",
    "yr_pred = svm.predict(X_test_std)                    # predição no teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcula os residuais\n",
    "resid_train_r = yr_train - svm.predict(Xr_train)\n",
    "print('Residual Treinamento RS =',sse_rs(resid_train_r))\n",
    "resid_test_r = yr_test - svm.predict(Xr_test)\n",
    "print('Residual Teste RS =',sse_rs(resid_test_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternativa: medir o ajuste do modelo para o dataset é R2 (R-squared score)\n",
    "print('Treinamento RS =',svm.score(Xr_train, yr_train))\n",
    "print('Test RS =',svm.score(Xr_test, yr_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matriz de confussão\n",
    "cm_rs = confusion_matrix(yr_test, yr_pred)\n",
    "#\n",
    "plt.figure(figsize=(6,4)) \n",
    "sns.heatmap(cm_rs.T, annot=True, cmap='gist_ncar_r')\n",
    "plt.title('Matriz de confusão usando SVM',y=1.05, size=15)\n",
    "plt.xlabel('Valores verdadeiros')\n",
    "plt.ylabel('Valores predecidos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### dataset: Vinho\n",
    ">> Dataset encontrado no [Kaggle](https://www.kaggle.com/datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vinho\n",
    "vinho = pd.read_csv('data/wine.csv')\n",
    "vinho.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transformacoes\n",
    "Xv = vinho.iloc[:,1]                       # alcool\n",
    "Xv = np.array(Xv)\n",
    "# matriz exemplos x caracteristicas\n",
    "Xvinho =Xv[:,np.newaxis]\n",
    "# objetivos\n",
    "yv= vinho.iloc[:,0]                       # classes 1,2,3\n",
    "yv = np.array(yv)\n",
    "yvinho=yv[:,np.newaxis]\n",
    "yvinho = np.ravel(yvinho) #\n",
    "#print(yvinho.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estatisticas básica\n",
    "#vinho.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot alcool\n",
    "sns.histplot(Xvinho, kde=1, color='red', label='Alcohol')\n",
    "plt.legend()\n",
    "plt.xlabel('Álcool')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot classes\n",
    "sns.histplot(yvinho, kde=1, color='red',label='Classes')\n",
    "plt.legend()\n",
    "plt.xlabel('Classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribuicao de alcool por classe\n",
    "figvinho = plt.figure(figsize=(8,6))\n",
    "sns.histplot(vinho['Alcohol'][yvinho==1],kde=1,label='Classe 1', color='red')\n",
    "sns.histplot(vinho['Alcohol'][yvinho==2],kde=1,label='Classe 2',color='blue')\n",
    "sns.histplot(vinho['Alcohol'][yvinho==3],kde=1,label='Classe 3',color='green')\n",
    "plt.ylabel('Classes')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#figvinho.savefig('images/classes_vinho.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cortando os dados\n",
    "Xv_train, Xv_test, yv_train, yv_test = train_test_split(Xvinho, yvinho, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adicionando metricas\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#\n",
    "modelo_vinho = DecisionTreeClassifier(random_state=1)     # cria o modelo de Arvore binaria\n",
    "#\n",
    "vinho_fit = modelo_vinho.fit(Xv_train, yv_train)          # ajusta aos valores do modelo treinamento\n",
    "#\n",
    "#Xv_pred = vinho_fit.predict(Xv_train)                     # predice sobre o treinamento\n",
    "yv_pred = vinho_fit.predict(Xv_test)                      # predice sobre o teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualzia o resporte de metricas\n",
    "metricas = classification_report(yv_test, yv_pred)\n",
    "print(metricas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcula os residuais\n",
    "def sse_v(residual):\n",
    "    return np.sum(np.power(residual,2))\n",
    "#\n",
    "resid_train_v = yv_train - modelo_vinho.predict(Xv_train)\n",
    "print('Residual Treinamento RS =',sse_v(resid_train_v))\n",
    "resid_test_v = yv_test - modelo_vinho.predict(Xv_test)\n",
    "print('Residual Teste RS =',sse_v(resid_test_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando R2 (R-squared score)\n",
    "print('Treinamento Vinho =',modelo_vinho.score(Xv_train, yv_train))\n",
    "print('Teste Vinho =',modelo_vinho.score(Xv_test, yv_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula a acurácia\n",
    "#ac_vinho = accuracy_score(yv_test, vinho_fit.predict(Xv_test))\n",
    "ac_vinho = accuracy_score(yv_test, yv_pred)\n",
    "print('Accuracy score = {:.3}'.format(ac_vinho*100),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matriz de confusão\n",
    "cm_v = confusion_matrix(yv_test, yv_pred)\n",
    "#\n",
    "plt.figure(figsize=(6,4)) \n",
    "sns.heatmap(cm_v.T, annot=True, cmap='gist_ncar_r')\n",
    "plt.title('Matriz de confusão usando DecisionTree',y=1.05, size=15)\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Classes predecidas')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Aprendizado Não-Supervisionado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Clusterização\n",
    ">> O processo de clusterização (agrupamento) permite classificar em conjuntos os dados semelhantes usando alguma característica comum a eles. Neste processo não é fornecido nenhum treinamento para descobrir os agrupamentos existentes nos dados. Os grupos gerados por essa classificação são chamados clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Dataset: Consumidor - Shooping\n",
    ">> Dataset encontrado no [Kaggle](https://www.kaggle.com/kondapuramshivani/mall-customerscsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clientes de shopping\n",
    "shopping_data = pd.read_csv('data/Mall_customers.csv')\n",
    "shopping_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deseja-se agrupar os clientes de acordo com sua receita anual e pontuação de gastos no shopping.\n",
    "\n",
    "O objetivo é encontrar os clusters (conjuntos) e ajudar o departamento de marketing a\n",
    "formular suas estratégias de venda. Por exemplo, podemos subdividir os clientes em 5 grupos distintos:\n",
    "1. Renda anual média, pontuação média de gastos\n",
    "2. Renda anual alta, baixa pontuação de gasto\n",
    "3. Baixa renda anual, baixa pontuação de gastos\n",
    "4. Baixa renda anual, alta pontuação de gastos\n",
    "5. Alta renda anual, alta pontuação de gastos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerandos os conjuntos de Treinamento e teste : clusters (opcao 5)\n",
    "Xc = shopping_data.iloc[:, [3, 4]].values\n",
    "#print(Xc)\n",
    "print(Xc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calacula o numero de clustes: metodo ELBOW\n",
    "from sklearn.cluster import KMeans\n",
    "warnings.filterwarnings('ignore')\n",
    "#\n",
    "def calculate_wcss(data):\n",
    "    wcss = []\n",
    "    for i in range(1,11):\n",
    "        kmeans = KMeans(n_clusters=i)\n",
    "        kmeans.fit(X=data)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "        #\n",
    "    return wcss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando o WCSS\n",
    "wcss = calculate_wcss(Xc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gráfico WCSS (Within-Cluster Sum of Square)\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "plt.plot(range(1, 11), wcss, 'r', lw=2.0)\n",
    "plt.title('Método de Elbow')\n",
    "plt.xlabel('Número de clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa os clusters\n",
    "#sns.set()\n",
    "kmeans= KMeans(n_clusters = 5, init ='k-means++', max_iter=300, n_init = 10, random_state=0)\n",
    "#\n",
    "# prognóstico dos clusters\n",
    "y_kmeans = kmeans.fit_predict(Xc)        # ajusta os clusters\n",
    "#\n",
    "# figure\n",
    "cluster = plt.figure(figsize=(15,10))\n",
    "#\n",
    "plt.scatter(Xc[y_kmeans == 0, 0], Xc[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\n",
    "plt.scatter(Xc[y_kmeans == 1, 0], Xc[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\n",
    "plt.scatter(Xc[y_kmeans == 2, 0], Xc[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\n",
    "plt.scatter(Xc[y_kmeans == 3, 0], Xc[y_kmeans == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')\n",
    "plt.scatter(Xc[y_kmeans == 4, 0], Xc[y_kmeans == 4, 1], s = 100, c ='yellow', label = 'Cluster 5')\n",
    "#\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c = 'black', label = 'Centroides')\n",
    "plt.title('Clusters de consumidores')\n",
    "plt.xlabel('Ingresso Anual (R$)')\n",
    "plt.ylabel('Pontuação de Gastos (1-100)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#cluster.savefig('imagens/cluster.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar as etiquetas\n",
    "#print(kmeans.labels_)\n",
    "#print(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrica de escore para clusters\n",
    "from sklearn.metrics import silhouette_score\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "#\n",
    "escore = silhouette_score(Xc, kmeans.labels_)\n",
    "print('Silhouette score: {:.3}'.format(escore))\n",
    "print('Silhouette score: {:.3}'.format(escore*100),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar o Silhouette escore\n",
    "modelx = KMeans(5)\n",
    "visualizer = SilhouetteVisualizer(modelx, colors='yellowbrick')\n",
    "#\n",
    "visualizer.fit(Xc)    # Fit the data to the visualizer\n",
    "visualizer.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA - Análise de Componentes Principais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.set()\n",
    "aleatorio = np.random.RandomState(1)\n",
    "X_PCA = np.dot(aleatorio.rand(2, 2), aleatorio.randn(2, 200)).T\n",
    "plt.scatter(X_PCA[:, 0], X_PCA[:, 1],color='red')\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando a bibliotecas\n",
    "from sklearn.decomposition import PCA\n",
    "#\n",
    "# Principal Components Analysis\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definir a direção do vetor sobre os dados de entrada\n",
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o comprimento do quadrado do vetor\n",
    "print(pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desenhar o vetor\n",
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(color='blue',arrowstyle='->',linewidth=2,shrinkA=0, shrinkB=0)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar os dados\n",
    "plt.scatter(X_PCA[:, 0], X_PCA[:, 1], alpha=0.5,color='red')\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Imagem Jackalope - Processamento de imagens\n",
    ">> Imagem em [Jackalope](https://dx.doi.org/10.6084/m9.figshare.2067186.v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar imagem Jackalope\n",
    "from IPython.display import Image\n",
    "Image(filename='imagens/Jackalope.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura de uma imagem\n",
    "from numpy import mean, size\n",
    "from skimage.io import imread\n",
    "#\n",
    "A = imread(r'imagens/Jackalope.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensões da matriz da imagem\n",
    "print(np.shape(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# média dos valores das camadas\n",
    "A1 = np.mean(A,2)\n",
    "print(A1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "full_pc = np.size(A1, axis=1)\n",
    "print(full_pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decomposicao\n",
    "from sklearn import decomposition\n",
    "from pylab import *\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = range(0,250,50)\n",
    "#\n",
    "fig = plt.figure()\n",
    "#\n",
    "for i, num_pc in enumerate(components):\n",
    "    i+=1\n",
    "    pca = decomposition.PCA(n_components=num_pc)\n",
    "    pca.fit(A1)                                               # ajusta a imagem\n",
    "    #\n",
    "    Rec = pca.inverse_transform(pca.transform(A1))\n",
    "    #\n",
    "    ax = fig.add_subplot(2,3,i,frame_on=False)\n",
    "    #\n",
    "    # removing ticks\n",
    "    ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "    imshow(Rec)\n",
    "    title(str(num_pc) + ' PCs')\n",
    "    gray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Imagem Andres - Processamento de Imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar imagem Jackalope\n",
    "from IPython.display import Image\n",
    "Image(filename='imagens/Andres_z.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura de uma imagem\n",
    "from numpy import mean, size\n",
    "from skimage.io import imread\n",
    "#\n",
    "Andres = imread(r'imagens/Andres_z.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensões da matriz da imagem\n",
    "print(np.shape(Andres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# média dos valores das camadas\n",
    "Az = np.mean(Andres,2)\n",
    "print(Az)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Componentes principais\n",
    "Andres_pc = np.size(Andres, axis=1)\n",
    "print(Andres_pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decomposicao\n",
    "from sklearn import decomposition\n",
    "from pylab import *\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = range(0,900,100)\n",
    "#\n",
    "fig = plt.figure()\n",
    "#\n",
    "for i, num_pc in enumerate(components):\n",
    "    i+=1\n",
    "    pca = decomposition.PCA(n_components=num_pc)              # decomposicao em n# componentes\n",
    "    pca.fit(Az)                                               # ajusta a imagem\n",
    "    #\n",
    "    Rec = pca.inverse_transform(pca.transform(Az))            # transformacoes matriciais\n",
    "    #\n",
    "    ax = fig.add_subplot(2,6,i,frame_on=False)\n",
    "    #\n",
    "    # removing ticks\n",
    "    ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "    imshow(Rec)\n",
    "    title(str(num_pc) + ' PCs')\n",
    "    gray()\n",
    "#fig.savefig('imagens/andres_pca.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Análise da Qualidade de vinho Tinto\n",
    ">> Dataset encontrado em [Vinho Tinto](http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leitura do dataset\n",
    "vtinto = pd.read_csv('data/winequality-red.csv', sep=';')\n",
    "vtinto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# particionando o dataset\n",
    "Xt = vtinto.iloc[:,[0,1,2,3,4,5,6,7,8,9,10]].values   # Data\n",
    "y = vtinto.quality                                    # objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estandarizar os dados: z = (x-mu)/var\n",
    "Xt = preprocessing.StandardScaler().fit(Xt).transform(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graficar\n",
    "model_v = PCA()                                            # cria a instancia\n",
    "result = model_v.fit(Xt)                                   # ajusta ao modelo (preditor)\n",
    "Z = result.transform(Xt)                                   # Z = (x-mu)/var\n",
    "plt.plot(result.explained_variance_)                       # explica a varianca\n",
    "plt.ylabel('% de variança')\n",
    "plt.xlabel('Número de Componentes')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esplicacao Raio da variancia : 'explained_variance_ratio_'\n",
    "#\n",
    "ev = result.explained_variance_ratio_\n",
    "print(ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretacao do grafico anterior\n",
    "info = pd.read_csv('data/pca_wine.txt',sep=';')\n",
    "info.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar o numero de cluster usando k-means\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "#\n",
    "model = KMeans()\n",
    "visualizer = KElbowVisualizer(model, k=(1,11))          # k(1,11)-> numero de variaveis\n",
    "#\n",
    "visualizer.fit(Xt)        \n",
    "visualizer.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Cancer\n",
    ">> Dataset pre-instalado no scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando o dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "#\n",
    "cancer = load_breast_cancer()                          # carrega o dataset\n",
    "#\n",
    "scaler = StandardScaler()                              # modelo de estandarizacao\n",
    "scaler.fit(cancer.data)                                     # ajusta os dados ao modelo\n",
    "X_scaled = scaler.transform(cancer.data)               # transforma os dados  std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the first two principal components of the data\n",
    "pca_c = PCA(n_components=2)\n",
    "#                            \n",
    "pca_c.fit(X_scaled)                                # ajusta os valores ao modelo\n",
    "#\n",
    "X_pca = pca_c.transform(X_scaled)                  # transforma os dados em duas PCA\n",
    "print(\"Dimensão Original: {}\".format(str(X_scaled.shape)))\n",
    "print(\"Dimensão Reduzida: {}\".format(str(X_pca.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gera a cor e a figura\n",
    "#\n",
    "B = (cancer.target == 0)\n",
    "M = (cancer.target == 1)\n",
    "#\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "#\n",
    "plt.scatter(X_pca[B, 0], X_pca[B, 1], marker='s', c='r', s=60, label='B', edgecolor='r', alpha=0.8)\n",
    "plt.scatter(X_pca[M, 0], X_pca[M, 1], marker='o', c='b', s=60, label='M', edgecolor='b', alpha=0.8)\n",
    "plt.legend()\n",
    "#\n",
    "plt.title(\"Treinamento de dados de cancer projetados em subespaços 2D\")\n",
    "plt.xlabel(\"Primeira Componente Principal\")\n",
    "plt.ylabel(\"Segunda Componente Principal\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#fig.savefig('imagens/cancer_pca.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Dataset: Handwritten digits\n",
    ">> Dataset pre-instalado no scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA como reducao de dimensoes usando datasets: digitos manuscritos\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagens de 8x8 pixels\n",
    "pca = PCA(2) # project from 64 to 2 dimensions\n",
    "projected = pca.fit_transform(digits.data)\n",
    "print(digits.data.shape)\n",
    "print(projected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot PCA\n",
    "# labels\n",
    "t0 = (digits.target == 0)\n",
    "t1 = (digits.target == 1)\n",
    "t2 = (digits.target == 2)\n",
    "t3 = (digits.target == 3)\n",
    "t4 = (digits.target == 4)\n",
    "t5 = (digits.target == 5)\n",
    "t6 = (digits.target == 6)\n",
    "t7 = (digits.target == 7)\n",
    "t8 = (digits.target == 8)\n",
    "t9 = (digits.target == 9)\n",
    "#\n",
    "pca = plt.figure(figsize=(10,8))\n",
    "#\n",
    "plt.scatter(projected[t0, 0], projected[t0, 1],c='r', label='0',edgecolor='none', alpha=0.8)\n",
    "plt.scatter(projected[t1, 0], projected[t1, 1],c='g', label='1',edgecolor='none', alpha=0.8)\n",
    "plt.scatter(projected[t2, 0], projected[t2, 1],c='b', label='2',edgecolor='none', alpha=0.8)\n",
    "plt.scatter(projected[t3, 0], projected[t3, 1],c='k', label='3',edgecolor='none', alpha=0.8)\n",
    "plt.scatter(projected[t4, 0], projected[t4, 1],c='y', label='4',edgecolor='none', alpha=0.8)\n",
    "plt.scatter(projected[t5, 0], projected[t5, 1],c='peru', label='5',edgecolor='none', alpha=0.8)\n",
    "plt.scatter(projected[t6, 0], projected[t6, 1],c='c', label='6', edgecolor='none', alpha=0.8)\n",
    "plt.scatter(projected[t7, 0], projected[t7, 1],c='lime', label='7',edgecolor='none', alpha=0.8)\n",
    "plt.scatter(projected[t8, 0], projected[t8, 1],c='deeppink', label='8',edgecolor='none', alpha=0.8)\n",
    "plt.scatter(projected[t9, 0], projected[t9, 1],c='dodgerblue', label='9',edgecolor='none', alpha=0.8)\n",
    "#\n",
    "plt.legend()\n",
    "plt.xlabel('componente 1')\n",
    "plt.ylabel('componente 2')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "#pca.savefig('imagens/digits_pca.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Fundamentos para Ciência Dados &copy; Copyright 2020-2022, Sergio Serra & Jorge Zavaleta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
